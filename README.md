# LLM-Fine-Tuned-GPT-2
This repository contains a from‐scratch implementation for fine-tuning OpenAI’s GPT-2 “small” (124M parameters) on instruction-style prompt‐response data, effectively creating a lightweight ChatGPT-like model that understands and follows user instructions.

# In FineTuning spam classification
Trained the model to detect the whether the message is spam or not.
